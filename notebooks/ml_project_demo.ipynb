{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Project Demo\n",
    "\n",
    "This notebook demonstrates a complete ML pipeline including:\n",
    "- Data loading and exploration\n",
    "- Feature preprocessing\n",
    "- Model training and comparison\n",
    "- Model evaluation\n",
    "- Deployment simulation\n",
    "- Comprehensive visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data.data_loader import DataLoader\n",
    "from features.preprocessing import FeaturePreprocessor\n",
    "from models.model_trainer import ModelTrainer\n",
    "from models.model_deployment import ModelDeployment\n",
    "from visualization.visualizer import MLVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "loader = DataLoader('boston')  # Try 'wine' or 'breast_cancer' for different problems\n",
    "X_train, X_test, y_train, y_test, df = loader.load_and_split()\n",
    "\n",
    "print(f\"Dataset: {loader.dataset_name}\")\n",
    "print(f\"Problem Type: {loader.problem_type}\")\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataset Description:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = MLVisualizer()\n",
    "\n",
    "# Plot data distribution\n",
    "visualizer.plot_data_distribution(df, target_col='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = FeaturePreprocessor(loader.problem_type)\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "X_train_processed, X_test_processed = preprocessor.full_preprocessing_pipeline(\n",
    "    X_train, y_train, X_test,\n",
    "    scaler_type='standard',\n",
    "    feature_selection=True,\n",
    "    k_features=min(10, X_train.shape[1])\n",
    ")\n",
    "\n",
    "print(f\"Original Features: {X_train.shape[1]}\")\n",
    "print(f\"Processed Features: {X_train_processed.shape[1]}\")\n",
    "print(\"Applied: Scaling + Feature Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer(loader.problem_type)\n",
    "\n",
    "# Train all models\n",
    "model_results = trainer.train_all_models(X_train_processed, y_train, X_test_processed, y_test)\n",
    "\n",
    "# Display results\n",
    "print(trainer.generate_model_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "visualizer.plot_model_comparison(model_results, loader.problem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_info = trainer.get_best_model()\n",
    "best_model_name = best_model_info['name']\n",
    "best_predictions = model_results[best_model_name]['test_predictions']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best {best_model_info['metric']}: {best_model_info['score']:.4f}\")\n",
    "\n",
    "# Plot predictions vs actual for best model\n",
    "if loader.problem_type == 'regression':\n",
    "    visualizer.plot_predictions_vs_actual(y_test, best_predictions, best_model_name)\n",
    "else:\n",
    "    visualizer.plot_confusion_matrix(y_test, best_predictions, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance if available\n",
    "best_model = model_results[best_model_name]['model']\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_names = [f'feature_{i}' for i in range(len(best_model.feature_importances_))]\n",
    "    visualizer.plot_feature_importance(\n",
    "        best_model.feature_importances_, \n",
    "        feature_names, \n",
    "        best_model_name\n",
    "    )\n",
    "else:\n",
    "    print(f\"{best_model_name} does not provide feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Deployment Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for deployment\n",
    "deployment = ModelDeployment()\n",
    "\n",
    "# Package preprocessors\n",
    "preprocessors = {\n",
    "    'scaler': preprocessor.fitted_scaler,\n",
    "    'feature_selector': preprocessor.feature_selector,\n",
    "    'pca': preprocessor.pca\n",
    "}\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'dataset': loader.dataset_name,\n",
    "    'problem_type': loader.problem_type,\n",
    "    'best_model': best_model_info['name'],\n",
    "    'performance': best_model_info['score'],\n",
    "    'features': X_train.shape[1],\n",
    "    'training_samples': X_train.shape[0]\n",
    "}\n",
    "\n",
    "# Save model for deployment\n",
    "model_path = deployment.save_model_for_deployment(best_model, preprocessors, metadata)\n",
    "print(f\"Model saved for deployment: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test API simulation\n",
    "deployment.load_model_for_inference(model_path)\n",
    "\n",
    "# Test with a sample\n",
    "test_sample = X_test.iloc[0:1] if hasattr(X_test, 'iloc') else X_test[0:1]\n",
    "api_response = deployment.simulate_api_endpoint(test_sample)\n",
    "\n",
    "print(\"API Simulation Results:\")\n",
    "for key, value in api_response.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Health check\n",
    "health_status = deployment.health_check()\n",
    "print(f\"\\nHealth Check: {health_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on best model\n",
    "X_full = np.vstack([X_train_processed, X_test_processed])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "cv_results = trainer.cross_validate_model(X_full, y_full, best_model_name, cv=5)\n",
    "\n",
    "print(f\"Cross-Validation Results for {best_model_name}:\")\n",
    "print(f\"  Mean Score: {cv_results['mean_score']:.4f} ± {cv_results['std_score']:.4f}\")\n",
    "print(f\"  Individual Scores: {cv_results['all_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning on Random Forest\n",
    "if 'random_forest' in model_results:\n",
    "    print(\"Performing hyperparameter tuning on Random Forest...\")\n",
    "    \n",
    "    tuned_model, best_params, best_score = trainer.hyperparameter_tuning(\n",
    "        X_train_processed, y_train, 'random_forest'\n",
    "    )\n",
    "    \n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best CV Score: {best_score:.4f}\")\n",
    "    \n",
    "    # Test tuned model\n",
    "    tuned_predictions = tuned_model.predict(X_test_processed)\n",
    "    \n",
    "    if loader.problem_type == 'regression':\n",
    "        from sklearn.metrics import r2_score\n",
    "        tuned_score = r2_score(y_test, tuned_predictions)\n",
    "        original_score = model_results['random_forest']['test_r2']\n",
    "        print(f\"Tuned Model Test R²: {tuned_score:.4f}\")\n",
    "        print(f\"Original Model Test R²: {original_score:.4f}\")\n",
    "        print(f\"Improvement: {tuned_score - original_score:.4f}\")\n",
    "    else:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        tuned_score = accuracy_score(y_test, tuned_predictions)\n",
    "        original_score = model_results['random_forest']['test_accuracy']\n",
    "        print(f\"Tuned Model Test Accuracy: {tuned_score:.4f}\")\n",
    "        print(f\"Original Model Test Accuracy: {original_score:.4f}\")\n",
    "        print(f\"Improvement: {tuned_score - original_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {loader.dataset_name} ({loader.problem_type})\")\n",
    "print(f\"Original Features: {X_train.shape[1]}\")\n",
    "print(f\"Processed Features: {X_train_processed.shape[1]}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")\n",
    "print(f\"Models Trained: {len(model_results)}\")\n",
    "print(f\"Best Model: {best_model_info['name']}\")\n",
    "print(f\"Best {best_model_info['metric']}: {best_model_info['score']:.4f}\")\n",
    "print(\"Deployment: Model packaged and API simulation tested\")\n",
    "print(\"Visualizations: Generated comprehensive plots\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}